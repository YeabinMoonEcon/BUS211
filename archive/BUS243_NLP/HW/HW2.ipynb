{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "\n",
    "Through the textbook, you'd build the TF-IDF transformer manually. You can find a similar result when you use the scikit-learn package (see section 3.4.3), but the results look somewhat different. Supply your intuition in words about what makes them different."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2\n",
    "In section 3.4.2, we measure the cosine similarity between the query (“How long does it take to get to the store?”) and the given documents. Please do the same exercise using the sklearn TfidfVectorizer function. \n",
    "\n",
    "    - Please use the same consine_sim function when you check the similarities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def cosine_sim(vec1, vec2):\n",
    "    \"\"\"\n",
    "    Since our vectors are dictionaries, lets convert them to lists for easier mathing.\n",
    "    \"\"\"\n",
    "    vec1 = [val for val in vec1.values()]\n",
    "    vec2 = [val for val in vec2.values()]\n",
    "    \n",
    "    dot_prod = 0\n",
    "    for i, v in enumerate(vec1):\n",
    "        dot_prod += v * vec2[i]\n",
    "        \n",
    "    mag_1 = math.sqrt(sum([x**2 for x in vec1]))\n",
    "    mag_2 = math.sqrt(sum([x**2 for x in vec2]))\n",
    "    \n",
    "    return dot_prod / (mag_1 * mag_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.42662402 0.18698644 0.18698644 0.\n",
      "  0.22087441 0.18698644 0.         0.         0.         0.18698644\n",
      "  0.         0.74794576 0.18698644 0.18698644]\n",
      " [0.46312056 0.         0.35221512 0.         0.         0.35221512\n",
      "  0.27352646 0.         0.35221512 0.35221512 0.         0.\n",
      "  0.46312056 0.         0.         0.        ]\n",
      " [0.         0.75143242 0.         0.         0.         0.28574186\n",
      "  0.22190405 0.         0.28574186 0.28574186 0.37571621 0.\n",
      "  0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "doc_0 = \"The faster Harry got to the store, the faster Harry, the faster, would get home.\"\n",
    "doc_1 = \"Harry is hairy and faster than Jill.\"\n",
    "doc_2 = \"Jill is not as hairy as Harry.\"\n",
    "\n",
    "corpus = [doc_0, doc_1, doc_2]\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df=1)\n",
    "model = vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(model.todense()) \n",
    "# you may need to convert the this outcome vector into a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How long does it take to get to the store?\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3\n",
    "\n",
    "Zipf’s law states that if the word types in a corpus are sorted by frequency, the frequency of the word rank at r is proportional to $\\frac{1}{r}$. In this question, let’s evaluate this law using Henry Wood Elliot’s book *Our Arctic province* (see `pg70373.txt`). \n",
    "\n",
    "Explain your finding."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4\n",
    "\n",
    "Data: The Movie Review Data is a collection of movie reviews from imdb.com in the early 2000s, curated by Bo Pang and Lillian Lee for their research on natural language processing. The dataset includes 2,000 reviews, evenly split between positive and negative sentiments. The data has been updated and cleaned up in v2.0, which was released in 2004. This dataset is often referred to as the polarity dataset.\n",
    "\n",
    "You need to develop a sentiment classifier using this dataset and evaluate its performance using cross-validation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) \n",
    "\n",
    "The first approach is to follow the methodology used in Homework 1:\n",
    "\n",
    "1. Split the Movie Review Data into training and test sets.\n",
    "2. Train a Naive Bayes model on the training set using a bag-of-words representation.\n",
    "3. Evaluate the model's performance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b)\n",
    "\n",
    "The next approach is to use TF-IDF representation and cosine similarity, similar to Question 2:\n",
    "\n",
    "1. Use the same split data in (a)\n",
    "2. Transform the training data into TF-IDF representation using the `TfidfVectorizer` function from the `sklearn` library (using `fit_transform()`).\n",
    "3. Transform the test data using `TfidfVectorizer.transform()`.\n",
    "4. For each review in the test set:\n",
    "    - Calculate the cosine similarity with the entire positive reviews in the training set.\n",
    "    - Calculate the cosine similarity with the entire negative reviews in the training set.\n",
    "    - Assign (predict) sentiment based on the higher cosine similarity.\n",
    "5. Evaluate the performance of the model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c)\n",
    "\n",
    "The last approach is to use Latent Semantic Analysis (`LSA`) with Linear Discriminant Analysis (`LDA`):\n",
    "\n",
    "1. Use the same split data in (a)\n",
    "2. Transform the training data into a document-topic representation using the `PCA` function from the `sklearn` library.\n",
    "3. Train a sentiment classifier using `LinearDiscriminantAnalysis` on the document-topic data.\n",
    "4. Evaluate the performance of the model.\n",
    "5. Repeat steps 2-4 with different numbers of topics.\n",
    "6. Determine the optimal number of topics by comparing the performance of the model with different numbers of topics.\n",
    "7. Justify the choice of the optimal number of topics based on the model's performance "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
