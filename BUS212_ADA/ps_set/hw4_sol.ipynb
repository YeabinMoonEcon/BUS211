{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4 Solution"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) \n",
    "- This approach involves randomly dividing the set of observations into k groups, or folds, of approximately equal size.\n",
    "-  The first fold is treated as a validation set, and the method is fit on the remaining $k−1$ folds\n",
    "-  The mean squared error, $MSE_1$, is then computed on the observations in the held-out fold\n",
    "- This procedure is repeated $k$ times; each time, a different group of observations is treated as a validation set\n",
    "- This process results in $k$ estimates of the test error, $MSE_1, MSE_2,..., MSE_k$\n",
    "- The k-fold CV estimate is computed by averaging these values:\n",
    "$$\\text{CV}_{(k)}=\\frac{1}{k}\\sum_{i=1}^{k}MSE_i$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2)\n",
    "\n",
    "The validation set has a clear computational advantage. However, the resulting test error estimates will be highly variable. It is not hard to see that LOOCV will give approximately unbiased estimates of the test error but has a higher variance, so that k-fold CV will give more accurate estimates."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It usually indicates the situation where you’d have more features than observations. I want you to think about why it might be a problem. You can raise the questions regarding the following:\n",
    "1.\tResource intensive: computation problems\n",
    "2.\tOverfitting: generalization problems"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "    The main motivations for dimensionality reduction are:\n",
    "    - To speed up a subsequent training algorithm (in some cases it may even remove noise and redundant features, making the training algorithm perform better)\n",
    "    - To visualize the data and gain insights on the most important features\n",
    "    - To save space (compression)\n",
    "    \n",
    "    The main drawbacks are:\n",
    "    - Some information is lost, possibly degrading the performance of subsequent training algorithms.\n",
    "    - It can be computationally intensive.\n",
    "    - It adds some complexity to your Machine Learning pipelines.\n",
    "    - Transformed features are often hard to interpret."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "\n",
    " Intuitively, a dimensionality reduction algorithm performs well if it eliminates a lot of dimensions from the dataset without losing too much information. One way to measure this is to apply the reverse transformation and measure the reconstruction error. However, not all dimensionality reduction algorithms provide a reverse transformation. Alternatively, if you are using dimensionality reduction as a preprocessing step before another Machine Learning algorithm (e.g., a Random Forest classifier), then you can simply measure the performance of that second algorithm; if dimensionality reduction did not lose too much information, then the algorithm should perform just as well as when using the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
