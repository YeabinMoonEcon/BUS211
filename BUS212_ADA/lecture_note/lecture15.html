<!doctype html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>BUS212:lecture15</title>

		<link rel="stylesheet" href="../../dist/reset.css">
		<link rel="stylesheet" href="../../dist/reveal.css">
		<link rel="stylesheet" href="../../dist/theme/white.css">

		<!-- Theme used for syntax highlighted code -->
		<link rel="stylesheet" href="../../plugin/highlight/monokai.css">
	</head>
	<body>
		<div class="reveal">
			<div class="slides">
				<section data-markdown>
					<textarea data-template>
						## Advanced Data Analytics
						## Lecture 15
						### Dimensionality Reduction
						
					</textarea>
				  </section>

<section>
	<h4>Announcement</h4>
<ul>
	<li class="fragment">?</li>
</ul>
</section>				   


<section>Most traditional statistical techniques for <br>regression and classification are
intended <br>for the low-dimensional setting</section>				   



<section>
	
	<ul>
		<li class="fragment">Traditional prediction problem</li>
		<ul>
			<li class="fragment">Blood pressure prediction model</li>
			<ul>
			<li class="fragment">Use three features:age, sex, BMI</li>
			</ul>
		</ul>	
		<li class="fragment">The number observation is much great than the number of features</li>
			<ul>
			<li class="fragment">$n>>p$, the problem is low-dimensional</li>
			</ul>
	</ul>
</section>


<section>
<h4>Examples</h4>	
	<ul>
		<li class="fragment">$p\approx 2,000$ for blood pressure prediction </li>
		<ul>
			<li class="fragment">out of $p\approx 600,000$</li>
			
		</ul>	
		<li class="fragment">Language models typically have more than 1,000 features</li>
			
	</ul>
</section>

<section>Data sets containing more features than observations are often referred
	to as <em>high-dimensional</em>, aka big data</section>
<section>Classical approaches such as least squares linear highdimensional regression are not appropriate</section>



<section>What's the problem in High Dimensions?</section>

<section>When the number of features $p$ is as large as, <br>or larger than, the number
	of observations $n$, <br>least squares cannot be performed</section>

<section>
	<img src="pics/pic15_1.png" width=690 class="fragment">	
	<ul>
		<li class="fragment">When $n>p$, ls line does not perfectly fit the data</li>
		<ul>
		<li class="fragment">the regression line seeks to approximate observations as much as possible</li>
		</ul>
	</ul>
</section>
	
<section>The problem is simple: when $p>n$ or $p \approx n$, <br> a simple least squares regression line is too flexible and hence overfits the data</section>

<section>
	See the following: <a href="https://colab.research.google.com/drive/1btTq6EC6zhYf_rSszFQ22EVCVLvNAxYZ?usp=sharing" target="_blank" >link</a>
</section>

<section>
	<img src="pics/pic15_2.png" width=313 class="">	
	<img src="pics/pic15_3.png" width=313 class="">	
	<img src="pics/pic15_4.png" width=313 class="">	
	<ul>
		<li class="fragment">$n=20$ training obs, $n=20$ test obs</li>
	</ul>
</section>



<section>Many of the methods seen earlier for fitting
	less flexible least squares models (ridge regresion, the lasso) are particularly
	useful for performing <br>regression in the high-dimensional setting</section>


<section>Let's go back to the code</section>

<section>
	<ul>
		<li class="fragment">Takeaway?</li>
		<ol>
			<li class="fragment">Regularization or shrinkage plays a key role in high-dimensional problems</li>
			<li class="fragment">Appropriate tuning parameter selection is crucial for good predictive performance</li>
			<li class="fragment">The test error tends to increase as the dimensionality of the problem increases, unless the additional
				features are truly associated with the response $\rightarrow$ curse of dimensionality</li>
		</ol>
	</ul>
</section>

<section> In general, adding additional signal features that are truly associated with the response will improve the fitted model,
	in the sense of leading to a reduction in test set error</section>

<section>However, adding noise features that are not truly associated with the response will lead
	to a deterioration in the fitted model, <br>and consequently an increased test
	set error</section>

<section>Alright. We've done for linear models.<br>Is there any general approach?</section>

<section>Consider the image precessing</section>
<section>
	<h4>Too heavy image?</h4>
	<ul>
		<li class="fragment">Increase compression rate</li>
		<ul>
			<li class="fragment">Raw vs. JPEG</li>
			<li class="fragment">limit the range of the values</li>
		</ul>
		<li class="fragment">Cut the unnecessary parts of images</li>
		<ul>
			<li class="fragment">delete some features</li>
		</ul>
	</ul>
</section>


<section>
		<h2>Feature transformation</h2>
		<table>
			<thead><tr>
				<th></th>
				<th>feature1</th>
				<th>feature2</th>
				<th>feature3</th>
			</tr></thead>
			<tbody><tr>
				<td>ind 1</td>
				<td>1</td>
				<td>0</td>
				<td>2</td>
			</tr>
			<tbody><tr>
				<td>ind 2</td>
				<td>10</td>
				<td>2</td>
				<td>22</td>
			</tr>
			<tbody><tr>
				<td>ind 3</td>
				<td>0</td>
				<td>1</td>
				<td>1</td>
			</tr></tbody>
		</table>
		<ul>
			<li class="fragment">See: $\text{feature 3} = \text{fature 1}\times 2+\text{feature 2} $</li>
			<li class="fragment">Do we have to keep feature 3?</li>
			<li class="fragment">If not, how to find feature 3?</li>
		</ul>
	</section>


</div>	

		<script src="../../dist/reveal.js"></script>
		<script src="../../plugin/notes/notes.js"></script>
		<script src="../../plugin/markdown/markdown.js"></script>
		<script src="../../plugin/highlight/highlight.js"></script>
		<script src="../../plugin/math/math.js"></script>
		<script>
			// More info about initialization & config:
			// - https://revealjs.com/initialization/
			// - https://revealjs.com/config/
			Reveal.initialize({
				hash: true,
				progress: false,
				touch: true,
				slideNumber: true,

				// Learn about plugins: https://revealjs.com/plugins/
				plugins: [ RevealMarkdown, RevealHighlight, RevealNotes, RevealMath.KaTeX ]
			});
		</script>
	</body>
</html>
