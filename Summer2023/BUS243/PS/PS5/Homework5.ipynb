{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this homework, I recommend utilizing **Google Colab**; however, you also have the option to submit the Jupyter Notebook. Prior to submission, please ensure that you have verified the proper execution of all code segments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If you try to load the data on Google Colab, it might cause your session to stop unexpectedly. So, please make sure to do Part 0 on your own computer. You don’t have to include Part 0 in your Colab submission.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall delve into the Yelp data to enhance our comprehension of it. Our objective involves training a deep neural network sentiment classifier.\n",
    "\n",
    "First, download the JSON Data from the following link: https://www.yelp.com/dataset/download. The total file size is approximately 9 GB, thus ensure your local machine has sufficient storage capacity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect `yelp_academic_dataset_review.json` data. Use the following below. Note that We only need `stars` and `text` columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def convert_large_json_to_dataframe(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            json_obj = json.loads(line)\n",
    "            data.append(json_obj)\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1**. Check the number of observations present in the dataset. It should be around 7,000,000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2**. Let's do simple data mining. \n",
    "\n",
    "- Employ the `sample()` method on the Pandas dataframe to extract a random 1 percent sample of observations. Then, save the extracted sample to a variable named `df`. Also, extract a random 1,000 sample and save it to a variable name `test_df`.\n",
    "- Convert the `stars` values into binary format, assigning a value of one if the rating is greater than or equal to 4.\n",
    "- Save the `train_df` and `test_df` dataframes to your computer, and then upload and use them in Google Colab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I: Learn a word embedding while fitting a neural net classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to learn a word embedding, it is essential to estimate the vocabulary size for the entire population data. However, it is unlikely to see the population data. Even JSON data you’ve downloaded is a sample. Here,  we need to estimate it using `train_df` dataframe.\n",
    "\n",
    "Let's figure out the size of vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simply the process:\n",
    "\n",
    "1. Convert all words to lowercase after tokenization.\n",
    "2. Eliminate stop words and punctuation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, extract a random 1,000 sample from `train_df` and save it to a variable name `small_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_df = train_df.sample(1000, replace= False, random_state = 123123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3.** What is the number of tokens in `small_df`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4.** What is the number of distinct tokens in in `small_df`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's go back to Heaps' law we learned in lecture 2:\n",
    "$$M=kT^b$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q5.** Explain the Heaps' law in words "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q6.** Now extract a random 3,000 sample from `train_df` and repeat **Q3** and **Q4**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q7**. Calculate $k$ and $b$ using **Q3, Q4, Q6**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q8.** Estimate the vocabualry size for `train_df` using Heap's law ($k$ and $b$)\n",
    "    - You have to count the number of tokens in `train_df`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del small_df1, small_df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's learn a word embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `input_dim`: set the size of the vocabulary you found in **Q8**, or 59,000 (choose the lower one). Remember to round this value to the nearest integer.\n",
    "- `output_dim`: set 100.\n",
    "- `input_length`: use the length of the longest content in the `text` column of `train_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = train_df.stars.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q9.** Train the model using `model.add(tf.keras.layers.Dense(1, activation='sigmoid'))` in the last layer. Try to achieve the highest possible levle of accuracy (at least **99% training accuracy**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q10.** Evaluate the model with `test_df` using accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run `tf.keras.backend.clear_session()` to clear all sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: Use Pre-Trained GloVe Embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q11.** Train and evaluate the model.\n",
    "\n",
    "- Use `glove.6B.100d.txt` from http://nlp.stanford.edu/data/glove.6B.zip\n",
    "- Try to get the accuracy as high as possible (at least 88.5 pecent).\n",
    "- Evaluate the model with `test_df` using accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q12.** Provide your intuitive assessment of the advantages and disadvantages associated with both approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
