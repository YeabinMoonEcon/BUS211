{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I\n",
    "Read the following article: *Cohen, Lauren, Christopher Malloy, and Quoc Nguyen. \"Lazy prices.\" The Journal of Finance 75.3 (2020): 1371-1415.* You can access this article through the Brandeis Library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. A 10-K report is an annual filing that publicly traded companies submit to the United States Securities and Exchange Commission (SEC). What kind of information do authors aim to gather from the 10-K report?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. What leads authors to the conclusion that there is a lack of an announcement effect in the 10-K report?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.\tWhy do authors emphasize not just the total word count but also its variations over time?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.\tWhen authors notice a substantial increase in the word count, why do they interpret it as a negative sentiment rather than a positive one?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II\n",
    "\n",
    "Let's explore the Yelp data and gain a better understanding of it. The data comprises five JSON files. Now, let's delve into some conceptual questions related to the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\tYou can find data description (metada) online with a few example: https://www.yelp.com/dataset/documentation/main. Explain the purpose of each file your own words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.\tData descriptions (metadata) can be found online, including a few examples. Explain the purpose of each file in your own words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.\tIn addition to sentiment analysis, the Yelp dataset can be utilized to address a range of other business inquiries. Supply your intuition on its potential usages within 200 words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.\tLet's narrow down our focus to sentiment analysis. The review text serves as the input, while the users provide the target outcome as stars in `review.json`. In other datasets, what additional features would be pertinent? How do these features contribute to estimating the target?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part III"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now utilize the sample.csv file, which is a subset of the yelp_academic_dataset_review.json dataset. In this section, we will construct a variety of classifiers and fine-tune them to predict user ratings based on the content of their reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### section 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-Learn provides a few functions to split datasets into multiple subsets in various ways. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1**. One of the parameter is called `random_state` for `train_test_split` method. Supply your intuition of its role."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2.** Split the data with `test_size=0.1`, and supply your intuition of test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3.** Prior to representing a text as a Bag of Words, it is essential to comprehend the underlying data. To achieve this, examine your training data and identify the row with the longest content in the text column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4.** Tokenize the text content obtained in **Q3** by using the `split()` function, and then display the top 10 tokens that are most frequently used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### section 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that the textual data in this dataset does not require any domain-specific language processing. Therefore, using a special tokenizer is unnecessary. Additionally, since each text is not lengthy, normalizing the text would be beneficial to ensure consistent and reliable analysis (think about how normalizing can help when dealing with shorter texts)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will construct a Na√Øve Bayes classifier using the MultinomialNB algorithm. You can utilize the code we previously employed in class for the data, but this time, replace GaussianNB with MultinomialNB. Afterwards, evaluate the model's performance using precision and recall metrics with the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### section 3-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will train a logistic regression model on the training set. First, we will utilize the bag-of-words representation that we transformed in section 2. Following that, we will evaluate the model's performance using precision and recall metrics with the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### section 3-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, instead of employing the entire vocabulary as features, we will use the following specific feature set to train the logistic regression model:\n",
    "\n",
    "- $x_1:$ tne number of positive words in a document\n",
    "- $x_2:$ the number of negative words in a document\n",
    "- $x_3:$ binary variable in which the value 1 indicates the presence of the word \"no\" in a document\n",
    "- $x_4:$ binary variable in which the value 1 indicates the presence of the symbol \"!\" in a document\n",
    "- $x_5: \\ln(\\text{word count of a document})$ \n",
    "\n",
    "Train the model using these features on the training set, and subsequently, assess the model's performance by evaluating precision and recall metrics with the test data."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
